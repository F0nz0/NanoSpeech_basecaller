# import needed modules
import os, sys, pysam
from __init__ import __version__
import argparse
from datetime import datetime
from tqdm import tqdm
import pandas as pd
from misc import phred_score_to_symbol


def retrieve_depth_stranded(bam_file, region, pos1based, strand):
    pos0based=pos1based-1
    depth_stranded = 0 # in case of no reads aligned
    for pileupcolumn in bam_file.pileup(region, pos0based, pos0based+1, truncate=True, max_depth=1000000, min_base_quality=0):
        column = pileupcolumn.get_query_sequences(mark_matches=True, add_indels=True)
        if strand == "+":
            depth_stranded = sum(1 for b in column if b.isupper())
        elif strand == "-":
            depth_stranded = sum(1 for b in column if b.islower())
    return depth_stranded


def retrieve_MODs_idx_fasta(fasta_filepath, read_name, return_readname=False):
    '''
    Extract from a fasta indexed file (using faih generated by *index_fasta_newlines* function)
    a list(s) of predicted modifications from the header line of the query read.
    '''
    faih_idx_file = fasta_filepath + ".faih"
    with open(faih_idx_file, "r") as idx:
        # detect read offset and len from idx file
        for r in idx:
            if r.split("\t")[0] == read_name:
                # retrieve offset and length for query read from the faih index file
                offset, length = int(r.rstrip().split("\t")[1]), int(r.rstrip().split("\t")[2])
                # retrieve MODIFICATION(s) idxs
                f = open(fasta_filepath, "rb")
                f.seek(offset, 0)
                line = f.read(length).decode("utf-8")[1:].strip().split(" ")
                if len(line) > 1:
                    rname = line[0]
                    idxs_dict = {}
                    for _ in line[1:]:
                        _m_id_ = _.split("=")[0].split("_")[0]
                        _m_idx_list = _.split("=")[1]
                        idxs_list = [int(i) for i in _m_idx_list.split(",")]
                        idxs_dict[_m_id_] = idxs_list
                else:
                    rname = line[0]
                    idxs_dict = {} # no modification detected for this read
                f.close()
                break
        if return_readname:
            return rname, idxs_dict
        else:
            return idxs_dict
        

def retrieve_MODs_idx_fastq(fastq_filepath, read_name, return_readname=False):
    '''
    Extract from a fastq indexed file (using faih generated by *index_fastq_newlines_quals* function)
    a list(s) of predicted modifications from the header line of the query read and the quality row.
    '''
    fqih_idx_file = fastq_filepath + ".fqih"
    with open(fqih_idx_file, "r") as idx:
        # detect read offset and len from idx file
        for r in idx:
            if r.split("\t")[0] == read_name:
                # retrieve offset, length, offset_qual and length_qual for query read from the fqih index file
                offset_head, length_head, offset_qual, length_qual = int(r.rstrip().split("\t")[1]), int(r.rstrip().split("\t")[2]), int(r.rstrip().split("\t")[3]), int(r.rstrip().split("\t")[4])
                # retrieve modifications idxs
                f = open(fastq_filepath, "rb")
                f.seek(offset_head, 0)
                line = f.read(length_head).decode("utf-8")[1:].strip().split(" ")
                if len(line) > 1:
                    rname = line[0]
                    idxs_dict = {}
                    for _ in line[1:]:
                        _m_id_ = _.split("=")[0].split("_")[0]
                        _m_idx_list = _.split("=")[1]
                        idxs_list = [int(i) for i in _m_idx_list.split(",")]
                        idxs_dict[_m_id_] = idxs_list
                else:
                    rname = line[0]
                    idxs_dict = {} # no modification detected for this read
                f.close()
                # retrieve modifications qualities
                quals_dict = {}
                for _ in idxs_dict.keys():
                    idxs_list = idxs_dict[_]
                    if len(idxs_list) != 0:
                        f = open(fastq_filepath, "rb")
                        f.seek(offset_qual, 0)
                        line = f.read(length_qual).decode("utf-8").rstrip()
                        quals_list = [line[i__] for i__ in idxs_list]
                        quals_dict[_] = quals_list
                        f.close()
                    else:
                        quals_list = []
                        quals_dict[_] = quals_list
                break
        if return_readname:
            return rname, idxs_dict, quals_dict
        else:
            return idxs_dict, quals_dict


# define new indexing for fasta headers-new_lines
def index_fasta_newlines(fasta_filepath):
    '''
    Function to create an .faih (fai like) index file for a fasta file generated by the basecaller.
    The index will be as tsv with 3 field:
    1) read-name
    2) offset in bytes (utf-8 encoding)
    3) length of header in bytes (utf-8 encoding)
    '''
    extention = os.path.splitext(fasta_filepath)[1][1:].lower()
    if extention == "fasta":
        new_line_char = ">"
    else:
        sys.exit("FORMAT ERROR! IT SHOULD BE a *.fasta!")
    output_idx_file = fasta_filepath+".faih" # fai like index for header lines offsets and lengths
    print(f"[{datetime.now()}] Saving header index file to:", output_idx_file, flush=True)
    with open(output_idx_file, "w") as out:
        with open(fasta_filepath, "rb") as f:
            byte_count = 0
            for i in f:
                len_byte = len(i.decode('utf-8'))
                if i.decode("utf-8").startswith(new_line_char):
                    # if line is a new line/header write index as a tsv file [readname_id  byte_offset  byte_length]
                    readname_id = i.decode("utf-8").rstrip()[1:].split(" ")[0]
                    offset = str(byte_count)
                    length = str(len_byte)
                    out.write( "\t".join([readname_id, offset, length])+"\n" )
                # update the byte counter
                byte_count += len_byte


# define new indexing for fastq headers-new_lines and qualities
def index_fastq_newlines_quals(fastq_filepath):
    '''
    Function to create an .fqih (fai like) index file for a fastq file generated by the basecaller.
    The index will be as tsv with 3 field:
    1) read-name
    2) offset in bytes for the header/new-line (utf-8 encoding)
    3) length of header in bytes (utf-8 encoding)
    4) offset in bytes for the qualities line (utf-8 encoding)
    5) length in bytes for the qualities line (utf-8 encoding)
    '''
    extention = os.path.splitext(fastq_filepath)[1][1:].lower()
    if extention == "fastq":
        new_line_char = "@"
    else:
        sys.exit("FORMAT ERROR! IT SHOULD BE a *.fastq!")
    output_idx_file = fastq_filepath+".fqih" # fai like index for header and qualities lines offsets and lengths
    print(f"[{datetime.now()}] Saving header index file to:", output_idx_file, flush=True)
    with open(output_idx_file, "w") as out:
        with open(fastq_filepath, "rb") as f:
            byte_count = 0
            last_line_start = ""
            for i in f:
                len_byte = len(i.decode('utf-8'))
                if i.decode("utf-8").startswith(new_line_char) and last_line_start != "+\n":
                    # if line is a new line/header write index as a tsv file [readname_id  byte_offset_head  byte_length_head  byte_offset_quality  byte_length_quality]
                    readname_id = i.decode("utf-8").rstrip()[1:].split(" ")[0]
                    offset_head = str(byte_count)
                    length_head = str(len_byte)
                elif last_line_start == "+\n":
                    offset_qual = str(byte_count)
                    length_qual = str(len_byte)
                    out.write( "\t".join([readname_id, offset_head, length_head, offset_qual, length_qual])+"\n" )
                # update the byte counter
                byte_count += len_byte
                last_line_start = i.decode("utf-8") # for fastq (quality line could start with @ symbol too)


# define modification calculator
def modification_detector(bam_filepath, fastx_filepath):
    '''
    Calculate the genomic positions of modifications called by NanoSpeech.
    It needs as input the filepath for the indexed bam and fasta/fastq files (files with modifications idx into the header 
    with .faih index).
    Iterate over each read into the bam file and performs the following operations:
    1) take the read name from the bam file
    2) extract modifications idx of the corresponding read/record from the fasta/fastq file.
    3) by the means of the MD tag it retrieves the genomic coordinate for each modified nucleotide and save it into an output file with 
       this format:
           region, start_position (0based), end_position (1based), readname, quality_score (for fasta == 0), strand (+,- or .)
    '''
    fastx_extention = os.path.splitext(fastx_filepath)[1].lower()

    output_per_read_filepath = bam_filepath + ".per_read.bed"
    with open(output_per_read_filepath, "w") as out:
        with pysam.AlignmentFile(bam_filepath) as bam:
            with tqdm(total=bam.mapped) as pbar:
                for r in bam.fetch():
                    if not r.is_unmapped:
                        pbar.update(1)
                        qname = r.qname
                        region = r.reference_name
                        # detect strand
                        if r.is_reverse:
                            strand = "-"
                        else:
                            strand = "+"
                        # infer read length
                        read_len = r.infer_read_length()
                        # detect if the read has hard clipping
                        hclip = r.get_cigar_stats()[0][5] 
                        if hclip > 0:
                            # detect if the hard clipping occurs on 5' or 3' end
                            if r.cigartuples[0][0] == 5:
                                # is on 5'
                                hclip = r.cigartuples[0][1]
                            else:
                                # is on 3' (we don't need to consider to retrieve aligned modifications indexes --> set to 0)
                                hclip = 0
                        # retrieve MODIFICATIONS idxs from fasta or fastq file
                        if fastx_extention == ".fasta":
                            MODSidxs = retrieve_MODs_idx_fasta(fastx_filepath, qname)
                            # create dummy qualities for each mod list
                            quals_dict = {}
                            for _k_ in MODSidxs.keys():
                                idxs = MODSidxs[_k_]
                                quals_dict[_k_] = [0 for i in range(len(idxs))]
                        elif fastx_extention == ".fastq":
                            MODSidxs, quals_dict = retrieve_MODs_idx_fastq(fastx_filepath, qname)
                        for _k_ in MODSidxs.keys():
                            idxs = MODSidxs[_k_]
                            quals_list = quals_dict[_k_]
                            # correct MODSidxs if read maps on reverse strand
                            if strand == "-":
                                idxs = [(read_len-1)-i for i in idxs]
                            # retrieve MD tag info and convert to a dataframe to convert fastx-mod indexes to bam coordinates
                            md = r.get_aligned_pairs(with_seq=True)
                            md_df = pd.DataFrame(md, columns=["qpos", "rpos", "ref_base"])
                            for idx,qual in zip(idxs, quals_list):
                                q = md_df.query(f"qpos == {idx-hclip}")
                                if not q.empty:
                                    refbase = (q.ref_base.iloc[0,])
                                    rpos = q.rpos.iloc[0,]
                                    if not pd.isnull(refbase):
                                        out.write(f"{region}\t{(rpos)}\t{rpos+1}\t{qname}\t{phred_score_to_symbol(qual)}\t{strand}\t{_k_}\t{refbase}\n")


# aggregate to genome space
def aggregate_genome_space(per_read_preds_filepath, bam_filepath, save_to_tsv=False, min_qual=None):
    # load per read table
    df = pd.read_table(per_read_preds_filepath, header=None, names=["region", "start", "stop", "read_name", "qual", "strand", "mod_base", "ref_base"])
    # filter by quality if requested
    if min_qual:
        df = df.query(f"qual >= {min_qual}")
    # aggregate to genome space counting modifications
    df_genome_space = df.groupby(["region", "start", "strand", "mod_base"]).size().to_frame(name="MOD_count").reset_index()
    del(df)
    # detect stranded depth
    with pysam.AlignmentFile(bam_filepath) as bam:
        depths = []
        with tqdm(total=df_genome_space.shape[0], desc="Calculating Stranded Depths...") as pbar:
            for pos in df_genome_space.itertuples():
                try:
                    depths.append( retrieve_depth_stranded(bam, str(pos.region), (pos.start)+1, pos.strand) )
                except:
                    print(f"[{datetime.now()}] ERROR! STOPPING!", flush=True)
                    print(f"[{datetime.now()}]", pos, flush=True)
                    break
                pbar.update(1)
    df_genome_space["depth"] = depths
    df_genome_space["MOD_freq"] = df_genome_space["MOD_count"] / df_genome_space["depth"]
    if save_to_tsv:
        output_filepath = bam_filepath + ".genome_space.tsv"
        df_genome_space.to_csv(output_filepath, index=False, sep="\t")
    else:
        return df_genome_space


# main function
def modifications_detector_main(bam_filepath, fastx_filepath, min_qual=None):
    '''
    Function to extract modifications positions on both read and genome-space dimensions.
    It is possible to use a minimum quality score for the called modifications when aggregating onto the
    genome-space the predictions.
    It takes in input the Bam, the fastx (fasta or fastq file generated by the basecaller) 
    and the threshold for the phred quality score of modifications (not required). 
    If the input fastx file is not indexed it will be indexed with custom ad-hoc functions.
    '''

    # detect fastx extention
    extention = os.path.splitext(fastx_filepath)[1][1:].lower()
    if not extention in ["fasta", "fastq"]:
        sys.exit(f"[{datetime.now()}] Input fastx extention not allowed ({extention}). It should be either fasta or fastq. Exiting...")
    print(f"[{datetime.now()}] Input fastx format detected: {extention}.", flush=True)
    if extention == "fasta":
        # asses if faih index file already exists
        fastx_index_filepath = fastx_filepath+".faih"
        if not os.path.exists(fastx_index_filepath):
            print(f"[{datetime.now()}] Producing new-lines index file for input fasta file.", flush=True)
            index_fasta_newlines(fastx_filepath)
    elif extention == "fastq":
        fastx_index_filepath = fastx_filepath+".fqih"
        if not os.path.exists(fastx_index_filepath):
            print(f"[{datetime.now()}] Producing new-lines index file for input fastq file.", flush=True)
            index_fastq_newlines_quals(fastx_filepath)

    # detect modifications to per read level
    print(f"[{datetime.now()}] Detecting modifications at per-read level.", flush=True)
    modification_detector(bam_filepath=bam_filepath, 
                          fastx_filepath=fastx_filepath)
    
    # infer expected per_read_preds_filepath bed/tsv filepath
    per_read_preds_filepath = bam_filepath + ".per_read.bed"
    print(f"[{datetime.now()}] Per-read prediction saved to file: {per_read_preds_filepath}", flush=True)

    print(f"[{datetime.now()}] Aggregating to genome space modifications predictions.")
    # aggregate onto genome-space using the required quality threshold (if provided)
    aggregate_genome_space(per_read_preds_filepath=per_read_preds_filepath, 
                           bam_filepath=bam_filepath, 
                           save_to_tsv=True, 
                           min_qual=min_qual)
    print(f"[{datetime.now()}] Genome-space aggregated tsv file saved to: {bam_filepath}.genome_space.tsv")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description=f"""NanoSpeech modification_detector.py v. {__version__}\nFunction to extract modifications positions on both read and genome-space dimensions.
    It is possible to use a minimum quality score for the called modifications when aggregating onto the
    genome-space the predictions.
    It takes in input the Bam, the fastx (fasta or fastq file generated by the basecaller) 
    and the threshold for the phred quality score of modifications (not required). 
    If the input fastx file is not indexed it will be indexed with custom ad-hoc functions.""")
    parser.add_argument("-b",
                        "--bam_filepath",
                        required=True,
                        type=str,
                        help="--bam_filepath: \t a <str> with the fullpath for the input BAM file.")
    parser.add_argument("-f",
                        "--fastx_filepath",
                        required=True,
                        type=str,
                        help="--fastx_filepath: \t a <str> with the fullpath for the input fasta/fastq file generated during the basecalling.")
    parser.add_argument("-q",
                        "--min_qual",
                        required=False,
                        default=None,
                        type=int,
                        help="--min_qual: \t a <int> used as minimum quality to filter modified basecalled bases/nucleotides during genome space aggregation step. [None]")

    args = parser.parse_args()
    bam_filepath = args.bam_filepath
    fastx_filepath = args.fastx_filepath
    min_qual = args.min_qual
    
    # print some starting info related to version, used program and to the input arguments
    print(f"[{datetime.now()}] NanoSpeech_basecaller version: {__version__}", flush=True)
    print(f"[{datetime.now()}] modification_detector.py Input arguments:", flush=True)
    for argument in args.__dict__.keys():
        print(f"\t- {argument} --> {args.__dict__[argument]}", flush=True)

    # launch main function
    modifications_detector_main(bam_filepath=bam_filepath,
                                fastx_filepath=fastx_filepath,
                                min_qual=min_qual)